#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ArXivè®ºæ–‡çˆ¬è™« - VLM/VLAç›¸å…³è®ºæ–‡è‡ªåŠ¨æŠ“å–å’Œæ€»ç»“
æ¯å¤©ä»arxivæŠ“å–VLM/VLAç›¸å…³è®ºæ–‡ï¼Œä½¿ç”¨Kimi APIæ€»ç»“ï¼Œç”ŸæˆJSONæ–‡ä»¶
"""

import arxiv
import requests
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
from setup_logging import get_logger

# è·å–æ—¥å¿—å™¨
logger = get_logger('arxiv_crawler')

class KimiAPIError(Exception):
    """Kimi APIè°ƒç”¨å¤±è´¥çš„è‡ªå®šä¹‰å¼‚å¸¸"""
    def __init__(self, paper_title: str, paper_id: str, error_code: int = None, error_message: str = None, pdf_url: str = None):
        self.paper_title = paper_title
        self.paper_id = paper_id
        self.error_code = error_code
        self.error_message = error_message
        self.pdf_url = pdf_url

        # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        msg = f"è®ºæ–‡å†…å®¹åˆ†æå¤±è´¥ - æ ‡é¢˜: '{paper_title}' (ID: {paper_id})"
        if pdf_url:
            msg += f", PDF URL: {pdf_url}"
        if error_code:
            msg += f", HTTPçŠ¶æ€ç : {error_code}"
        if error_message:
            msg += f", é”™è¯¯ä¿¡æ¯: {error_message}"

        super().__init__(msg)

class PaperProcessingError(Exception):
    """è®ºæ–‡å¤„ç†å¤±è´¥çš„è‡ªå®šä¹‰å¼‚å¸¸"""
    def __init__(self, paper_title: str, paper_id: str, original_error: Exception):
        self.paper_title = paper_title
        self.paper_id = paper_id
        self.original_error = original_error

        msg = f"è®ºæ–‡å¤„ç†å¤±è´¥ - æ ‡é¢˜: '{paper_title}' (ID: {paper_id}), åŸå› : {str(original_error)}"
        super().__init__(msg)

class ArxivPaperCrawler:
    def __init__(self, kimi_api_key: str, kimi_base_url: str = "https://api.moonshot.cn/v1"):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            kimi_api_key: Kimi APIå¯†é’¥
            kimi_base_url: Kimi APIåŸºç¡€URL
        """
        self.kimi_api_key = kimi_api_key
        self.kimi_base_url = kimi_base_url
        self.headers = {
            "Authorization": f"Bearer {kimi_api_key}",
            "Content-Type": "application/json"
        }

        # ä»é…ç½®è·å–æœç´¢å…³é”®è¯å’Œå»¶è¿Ÿè®¾ç½®
        from config import Config
        self.keywords = Config.get_search_keywords()
        self.search_topic = Config.SEARCH_TOPIC
        self.kimi_request_delay = Config.KIMI_REQUEST_DELAY
        self.paper_processing_delay = Config.PAPER_PROCESSING_DELAY
        self.min_relevance_score = Config.MIN_RELEVANCE_SCORE
        self.enable_verification = Config.ENABLE_VERIFICATION
        self.verification_delay = Config.VERIFICATION_DELAY
        self.max_verification_attempts = Config.MAX_VERIFICATION_ATTEMPTS

    def search_papers(self, days_back: int = 1) -> List[arxiv.Result]:
        """
        æœç´¢arxivä¸Šçš„ç›¸å…³è®ºæ–‡

        Args:
            days_back: æœç´¢è¿‡å»å‡ å¤©çš„è®ºæ–‡

        Returns:
            è®ºæ–‡ç»“æœåˆ—è¡¨
        """
        search_topic = self.search_topic if self.search_topic != "BOTH" else "VLM/VLA"
        logger.info(f"å¼€å§‹æœç´¢è¿‡å»{days_back}å¤©çš„{search_topic}ç›¸å…³è®ºæ–‡...")

        # æ„å»ºæœç´¢æŸ¥è¯¢
        query_parts = []
        for keyword in self.keywords:
            query_parts.append(f'all:"{keyword}"')

        query = " OR ".join(query_parts)

        # è®¾ç½®æ—¶é—´èŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        try:
            # æœç´¢è®ºæ–‡
            print(query)
            print(start_date)
            search = arxiv.Search(
                query=query,
                max_results=50,  # é™åˆ¶ç»“æœæ•°é‡
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )

            papers = []
            for paper in search.results():
                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                if paper.published.replace(tzinfo=None) >= start_date:
                    papers.append(paper)

            logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            return papers

        except Exception as e:
            logger.error(f"æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []

    def _call_kimi_api(self, prompt: str, title: str, paper_id: str) -> str:
        """
        è°ƒç”¨Kimi APIçš„åŸºç¡€æ–¹æ³•

        Args:
            prompt: æç¤ºè¯
            title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            paper_id: è®ºæ–‡IDï¼ˆç”¨äºé”™è¯¯æŠ¥å‘Šï¼‰

        Returns:
            APIè¿”å›çš„å†…å®¹

        Raises:
            KimiAPIError: å½“APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # åœ¨APIè¯·æ±‚å‰æ·»åŠ ç­‰å¾…ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            logger.debug(f"ç­‰å¾…{self.kimi_request_delay}ç§’ä»¥é¿å…APIé¢‘ç‡é™åˆ¶...")
            time.sleep(self.kimi_request_delay)

            response = requests.post(
                f"{self.kimi_base_url}/chat/completions",
                headers=self.headers,
                json={
                    "model": "moonshot-v1-32k",
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()

                # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«é”™è¯¯
                if 'error' in result:
                    error_msg = result['error'].get('message', 'æœªçŸ¥APIé”™è¯¯')
                    logger.error(f"APIè¿”å›é”™è¯¯ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
                    raise KimiAPIError(title, paper_id, response.status_code, error_msg)

                if 'choices' not in result or not result['choices']:
                    error_msg = "APIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œç¼ºå°‘choiceså­—æ®µ"
                    logger.error(f"APIå“åº”å¼‚å¸¸ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
                    raise KimiAPIError(title, paper_id, response.status_code, error_msg)

                content = result['choices'][0]['message']['content'].strip()

                # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
                if not content:
                    error_msg = "APIè¿”å›ç©ºå†…å®¹"
                    logger.error(f"APIè¿”å›ç©ºå†…å®¹ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                    raise KimiAPIError(title, paper_id, response.status_code, error_msg)

                return content
            else:
                # å¤„ç†HTTPé”™è¯¯
                error_msg = "HTTPè¯·æ±‚å¤±è´¥"
                try:
                    error_response = response.json()
                    if 'error' in error_response:
                        error_msg = error_response['error'].get('message', error_msg)
                except:
                    error_msg = f"HTTP {response.status_code}: {response.text[:200] if response.text else 'æ— å“åº”å†…å®¹'}"

                logger.error(f"Kimi APIè¯·æ±‚å¤±è´¥ - è®ºæ–‡: '{title}' (ID: {paper_id}), çŠ¶æ€ç : {response.status_code}, é”™è¯¯: {error_msg}")

                # æ ¹æ®çŠ¶æ€ç æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
                if response.status_code == 400:
                    error_msg += " (å¯èƒ½æ˜¯PDF URLæ— æ³•è®¿é—®æˆ–æ ¼å¼ä¸æ”¯æŒ)"
                elif response.status_code == 401:
                    error_msg += " (APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ)"
                elif response.status_code == 403:
                    error_msg += " (APIè®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½æ˜¯æƒé™ä¸è¶³)"
                elif response.status_code == 429:
                    error_msg += " (APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•)"
                elif response.status_code >= 500:
                    error_msg += " (æœåŠ¡å™¨å†…éƒ¨é”™è¯¯)"

                raise KimiAPIError(title, paper_id, response.status_code, error_msg)

        except requests.exceptions.Timeout:
            error_msg = "è¯·æ±‚è¶…æ—¶ï¼Œå¯èƒ½æ˜¯PDFæ–‡ä»¶è¿‡å¤§æˆ–ç½‘ç»œè¿æ¥ä¸ç¨³å®š"
            logger.error(f"APIè¯·æ±‚è¶…æ—¶ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
            raise KimiAPIError(title, paper_id, None, error_msg)

        except requests.exceptions.ConnectionError:
            error_msg = "ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œæ— æ³•è¿æ¥åˆ°Kimi APIæœåŠ¡å™¨"
            logger.error(f"ç½‘ç»œè¿æ¥å¤±è´¥ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
            raise KimiAPIError(title, paper_id, None, error_msg)

        except requests.exceptions.RequestException as e:
            error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
            logger.error(f"è¯·æ±‚å¼‚å¸¸ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
            raise KimiAPIError(title, paper_id, None, error_msg)

        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            logger.error(f"æœªçŸ¥é”™è¯¯ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
            raise KimiAPIError(title, paper_id, None, error_msg)

    def _verify_summary(self, paper_url: str, original_summary: Dict[str, str], title: str, paper_id: str) -> bool:
        """
        éªŒè¯ç”Ÿæˆçš„æ€»ç»“æ˜¯å¦å‡†ç¡®

        Args:
            paper_url: è®ºæ–‡PDF URL
            original_summary: åŸå§‹æ€»ç»“
            title: è®ºæ–‡æ ‡é¢˜
            paper_id: è®ºæ–‡ID

        Returns:
            éªŒè¯æ˜¯å¦é€šè¿‡
        """
        verification_prompt = f"""è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹URLä¸­çš„è®ºæ–‡å†…å®¹ï¼Œå¹¶éªŒè¯ç»™å‡ºçš„æ€»ç»“æ˜¯å¦å‡†ç¡®ï¼š

è®ºæ–‡URL: {paper_url}

å¾…éªŒè¯çš„æ€»ç»“ï¼š
ä¸­æ–‡æ€»ç»“ï¼š{original_summary['chinese_summary']}
è‹±æ–‡æ€»ç»“ï¼š{original_summary['english_summary']}

è¯·éªŒè¯ä»¥ä¸‹å‡ ç‚¹ï¼š
1. æ€»ç»“æ˜¯å¦åŸºäºè¯¥è®ºæ–‡çš„å®é™…å†…å®¹
2. æ€»ç»“æ˜¯å¦å‡†ç¡®åæ˜ äº†è®ºæ–‡çš„æ ¸å¿ƒé—®é¢˜ã€æ–¹æ³•å’Œè´¡çŒ®
3. æ€»ç»“ä¸­æ˜¯å¦åŒ…å«äº†è®ºæ–‡ä¸­æœªæåŠçš„å†…å®¹

è¯·å›ç­”ï¼š
éªŒè¯ç»“æœï¼šé€šè¿‡/ä¸é€šè¿‡
åŸå› ï¼š[å¦‚æœä¸é€šè¿‡ï¼Œè¯·è¯´æ˜å…·ä½“åŸå› ]"""

        try:
            logger.info(f"å¼€å§‹éªŒè¯è®ºæ–‡æ€»ç»“ - è®ºæ–‡: '{title}' (ID: {paper_id})")
            logger.debug(f"ç­‰å¾…{self.verification_delay}ç§’åè¿›è¡ŒéªŒè¯...")
            time.sleep(self.verification_delay)

            verification_content = self._call_kimi_api(verification_prompt, title, paper_id)

            # è§£æéªŒè¯ç»“æœ
            if "é€šè¿‡" in verification_content and "ä¸é€šè¿‡" not in verification_content:
                logger.info(f"âœ… éªŒè¯é€šè¿‡ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                return True
            else:
                logger.warning(f"âŒ éªŒè¯ä¸é€šè¿‡ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                logger.warning(f"éªŒè¯è¯¦æƒ…: {verification_content}")
                return False

        except Exception as e:
            logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {str(e)}")
            # éªŒè¯å¤±è´¥æ—¶é»˜è®¤è®¤ä¸ºé€šè¿‡ï¼Œé¿å…é˜»å¡æµç¨‹
            return True

    def summarize_with_kimi(self, paper_url: str, title: str, paper_id: str) -> Dict[str, str]:
        """
        ä½¿ç”¨Kimi APIé€šè¿‡è®ºæ–‡URLåˆ†æè®ºæ–‡å†…å®¹ï¼Œæ”¯æŒéªŒè¯å’Œé‡è¯•

        Args:
            paper_url: è®ºæ–‡PDF URL
            title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            paper_id: è®ºæ–‡IDï¼ˆç”¨äºé”™è¯¯æŠ¥å‘Šï¼‰

        Returns:
            åŒ…å«ä¸­è‹±æ–‡è®ºæ–‡æ€»ç»“çš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
            - chinese_summary: é—®é¢˜ã€æ–¹æ³•ã€è´¡çŒ®çš„ä¸­æ–‡æ¦‚æ‹¬
            - english_summary: é—®é¢˜ã€æ–¹æ³•ã€è´¡çŒ®çš„è‹±æ–‡æ¦‚æ‹¬

        Raises:
            KimiAPIError: å½“APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
        """
        # ä¼˜åŒ–çš„Kimiæç¤ºè¯ï¼Œè¦æ±‚ä¸¥æ ¼æŒ‰ç…§URLå†…å®¹è¿›è¡Œåˆ†æ
        url_prompt = f"""è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹URLä¸­çš„å®Œæ•´è®ºæ–‡å†…å®¹ï¼Œå¹¶ä¸¥æ ¼åŸºäºè¯¥è®ºæ–‡çš„å®é™…å†…å®¹è¿›è¡Œåˆ†æï¼š

è®ºæ–‡URL: {paper_url}

é‡è¦è¦æ±‚ï¼š
1. å¿…é¡»å®Œæ•´é˜…è¯»URLä¸­çš„è®ºæ–‡å…¨æ–‡
2. åªèƒ½åŸºäºè¯¥URLè®ºæ–‡çš„å®é™…å†…å®¹è¿›è¡Œæ€»ç»“
3. ä¸å¾—æ·»åŠ ä»»ä½•URLè®ºæ–‡ä¸­æœªæåŠçš„å†…å®¹

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›åˆ†æï¼š

ã€ä¸­æ–‡æ€»ç»“ã€‘
ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¯¥è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼Œæå‡ºçš„ä¸»è¦æ–¹æ³•å’Œå…³é”®è´¡çŒ®ï¼š

ã€English Summaryã€‘
Core problem solved, main method proposed and key contribution in one sentence:

æ³¨æ„ï¼šæ¯ä¸ªæ¦‚æ‹¬å¿…é¡»ä¸¥æ ¼åŸºäºURLè®ºæ–‡çš„å®é™…å†…å®¹ï¼Œä½¿ç”¨ç®€æ´æ˜ç¡®çš„ä¸€å¥è¯è¡¨è¾¾ï¼Œä¸å¾—è¶…å‡ºè®ºæ–‡èŒƒå›´ã€‚"""

        logger.info(f"æ­£åœ¨é€šè¿‡URLåˆ†æè®ºæ–‡å†…å®¹: {title} (ID: {paper_id})")
        logger.debug(f"PDF URL: {paper_url}")

        # å°è¯•ç”Ÿæˆå’ŒéªŒè¯æ€»ç»“
        for attempt in range(self.max_verification_attempts):
            try:
                logger.info(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•ç”Ÿæˆæ€»ç»“ - è®ºæ–‡: '{title}' (ID: {paper_id})")

                # è°ƒç”¨APIç”Ÿæˆæ€»ç»“
                content = self._call_kimi_api(url_prompt, title, paper_id)

                # è§£æä¸­è‹±æ–‡æ€»ç»“
                chinese_summary = ""
                english_summary = ""

                # å°è¯•æŒ‰åˆ†å‰²ç¬¦åˆ†å‰²å†…å®¹
                if 'ã€English Summaryã€‘' in content:
                    parts = content.split('ã€English Summaryã€‘')
                elif 'English Summary' in content:
                    parts = content.split('English Summary')
                else:
                    parts = [content, ""]

                if len(parts) >= 2:
                    # æå–ä¸­æ–‡éƒ¨åˆ†
                    chinese_part = parts[0].replace('ã€ä¸­æ–‡æ€»ç»“ã€‘', '').replace('ä¸­æ–‡æ€»ç»“', '').strip()
                    # æå–è‹±æ–‡éƒ¨åˆ†
                    english_part = parts[1].strip()

                    # æ¸…ç†å†…å®¹
                    chinese_summary = chinese_part if chinese_part else ""
                    english_summary = english_part if english_part else ""
                else:
                    # å¦‚æœæ— æ³•åˆ†å‰²ï¼Œå°†æ•´ä¸ªå†…å®¹ä½œä¸ºä¸­æ–‡æ€»ç»“
                    chinese_summary = content.replace('ã€ä¸­æ–‡æ€»ç»“ã€‘', '').replace('ä¸­æ–‡æ€»ç»“', '').strip()
                    english_summary = ""

                # æ£€æŸ¥è§£æç»“æœ
                if not chinese_summary and not english_summary:
                    if attempt < self.max_verification_attempts - 1:
                        logger.warning(f"è§£æå¤±è´¥ï¼Œå°†é‡è¯• - è®ºæ–‡: '{title}' (ID: {paper_id})")
                        continue
                    else:
                        error_msg = f"æ— æ³•è§£æAPIè¿”å›å†…å®¹ï¼ŒåŸå§‹å†…å®¹: {content[:200]}..."
                        logger.error(f"å†…å®¹è§£æå¤±è´¥ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
                        raise KimiAPIError(title, paper_id, None, error_msg, paper_url)

                # ç¡®ä¿æœ‰å†…å®¹
                chinese_summary = chinese_summary or "è§£æå¤±è´¥ï¼Œæ— æ³•è·å–è®ºæ–‡æ€»ç»“"
                english_summary = english_summary or "Parsing failed, unable to get paper summary"

                # æ„å»ºæ€»ç»“å­—å…¸
                summary_dict = {
                    "chinese_summary": chinese_summary,
                    "english_summary": english_summary
                }

                # å¦‚æœå¯ç”¨éªŒè¯åŠŸèƒ½ï¼Œè¿›è¡ŒéªŒè¯
                if self.enable_verification:
                    is_valid = self._verify_summary(paper_url, summary_dict, title, paper_id)
                    if is_valid:
                        logger.info(f"âœ… æ€»ç»“éªŒè¯é€šè¿‡ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                        return summary_dict
                    else:
                        if attempt < self.max_verification_attempts - 1:
                            logger.warning(f"ğŸ”„ éªŒè¯ä¸é€šè¿‡ï¼Œå°†é‡æ–°ç”Ÿæˆ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                            continue
                        else:
                            logger.warning(f"âš ï¸  éªŒè¯ä¸é€šè¿‡ä½†å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨å½“å‰ç»“æœ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                            return summary_dict
                else:
                    # æœªå¯ç”¨éªŒè¯ï¼Œç›´æ¥è¿”å›ç»“æœ
                    logger.info(f"æˆåŠŸåˆ†æè®ºæ–‡å†…å®¹ - è®ºæ–‡: '{title}' (ID: {paper_id})")
                    return summary_dict

            except KimiAPIError:
                # å¦‚æœæ˜¯APIé”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­é‡è¯•
                if attempt < self.max_verification_attempts - 1:
                    logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œå°†é‡è¯• - è®ºæ–‡: '{title}' (ID: {paper_id})")
                    continue
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise

        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
        error_msg = f"ç»è¿‡ {self.max_verification_attempts} æ¬¡å°è¯•ä»æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ€»ç»“"
        logger.error(f"ç”Ÿæˆæ€»ç»“å¤±è´¥ - è®ºæ–‡: '{title}' (ID: {paper_id}), é”™è¯¯: {error_msg}")
        raise KimiAPIError(title, paper_id, None, error_msg, paper_url)

    def _get_default_summary(self) -> Dict[str, str]:
        """è¿”å›é»˜è®¤çš„æ€»ç»“æ ¼å¼"""
        return {
            "chinese_summary": "APIè°ƒç”¨å¤±è´¥æˆ–PDFæ— æ³•è®¿é—®ï¼Œæ— æ³•åˆ†æè®ºæ–‡å†…å®¹",
            "english_summary": "API call failed or PDF inaccessible, unable to analyze paper content"
        }
    def process_papers(self, papers: List[arxiv.Result]) -> List[Dict[str, Any]]:
        """
        å¤„ç†è®ºæ–‡åˆ—è¡¨ï¼Œç”Ÿæˆæ ‡å‡†åŒ–çš„å­—å…¸æ ¼å¼

        Args:
            papers: arxivè®ºæ–‡ç»“æœåˆ—è¡¨

        Returns:
            æ ‡å‡†åŒ–çš„è®ºæ–‡ä¿¡æ¯å­—å…¸åˆ—è¡¨

        Raises:
            PaperProcessingError: å½“è®ºæ–‡å¤„ç†å¤±è´¥æ—¶æŠ›å‡º
        """
        processed_papers = []
        failed_papers = []
        filtered_papers = []  # è¢«è¿‡æ»¤æ‰çš„ä½ç›¸å…³æ€§è®ºæ–‡

        for i, paper in enumerate(papers):
            paper_id = paper.entry_id.split('/')[-1]
            paper_title = paper.title.strip()

            logger.info(f"å¤„ç†è®ºæ–‡ {i+1}/{len(papers)}: {paper_title} (ID: {paper_id})")

            # å…ˆè®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼Œè¿›è¡Œé¢„è¿‡æ»¤
            relevance_score = self._calculate_relevance_score(paper.title, paper.summary)
            logger.debug(f"è®ºæ–‡ç›¸å…³æ€§åˆ†æ•°: {relevance_score:.3f}")

            # å¦‚æœç›¸å…³æ€§åˆ†æ•°ä½äºé˜ˆå€¼ï¼Œè·³è¿‡å¤„ç†
            if relevance_score < self.min_relevance_score:
                logger.info(f"âš ï¸  è®ºæ–‡ç›¸å…³æ€§åˆ†æ•° {relevance_score:.3f} ä½äºé˜ˆå€¼ {self.min_relevance_score}ï¼Œè·³è¿‡å¤„ç†: {paper_title} (ID: {paper_id})")
                filtered_papers.append({
                    "paper_id": paper_id,
                    "paper_title": paper_title,
                    "relevance_score": relevance_score,
                    "reason": f"ç›¸å…³æ€§åˆ†æ•° {relevance_score:.3f} < {self.min_relevance_score}"
                })
                continue

            try:
                # ä½¿ç”¨Kimié€šè¿‡URLåˆ†æè®ºæ–‡å†…å®¹
                summary = self.summarize_with_kimi(paper.pdf_url, paper_title, paper_id)
                # æ„å»ºæ ‡å‡†åŒ–å­—å…¸
                paper_dict = {
                    "id": paper_id,
                    "title": paper_title,
                    "authors": [author.name for author in paper.authors],
                    "published_date": paper.published.strftime("%Y-%m-%d"),
                    "updated_date": paper.updated.strftime("%Y-%m-%d") if paper.updated else None,
                    "categories": paper.categories,
                    "primary_category": paper.primary_category,
                    "pdf_url": paper.pdf_url,
                    "arxiv_url": paper.entry_id,
                    "summary": {
                        "chinese_summary": summary["chinese_summary"],
                        "english_summary": summary["english_summary"]
                    },
                    "crawl_timestamp": datetime.now().isoformat(),
                    "relevance_score": relevance_score,
                    "processing_status": "success"
                }

                processed_papers.append(paper_dict)
                logger.info(f"âœ… æˆåŠŸå¤„ç†è®ºæ–‡: {paper_title} (ID: {paper_id}), ç›¸å…³æ€§: {relevance_score:.3f}")

            except KimiAPIError as e:
                # è®°å½•APIè°ƒç”¨å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
                logger.error(f"âŒ Kimi APIè°ƒç”¨å¤±è´¥: {str(e)}")
                failed_papers.append({
                    "paper_id": paper_id,
                    "paper_title": paper_title,
                    "error_type": "KimiAPIError",
                    "error_message": str(e),
                    "pdf_url": paper.pdf_url,
                    "relevance_score": relevance_score
                })

                # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å†³å®šå¦‚ä½•å¤„ç†
                raise PaperProcessingError(paper_title, paper_id, e)

            except Exception as e:
                # è®°å½•å…¶ä»–ç±»å‹çš„é”™è¯¯
                logger.error(f"âŒ è®ºæ–‡å¤„ç†å¤±è´¥: {paper_title} (ID: {paper_id}), é”™è¯¯: {str(e)}")
                failed_papers.append({
                    "paper_id": paper_id,
                    "paper_title": paper_title,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "pdf_url": paper.pdf_url,
                    "relevance_score": relevance_score
                })

                # æŠ›å‡ºå¼‚å¸¸
                raise PaperProcessingError(paper_title, paper_id, e)

            # åœ¨å¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡å‰æ·»åŠ é¢å¤–å»¶è¿Ÿ
            if i < len(papers) - 1:  # ä¸æ˜¯æœ€åä¸€ç¯‡è®ºæ–‡
                logger.debug(f"å¤„ç†å®Œæˆï¼Œç­‰å¾…{self.paper_processing_delay}ç§’åå¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡...")
                time.sleep(self.paper_processing_delay)

        # è®°å½•å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        total_found = len(papers)
        total_processed = len(processed_papers)
        total_filtered = len(filtered_papers)
        total_failed = len(failed_papers)

        logger.info(f"ğŸ“Š è®ºæ–‡å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  - æ€»æ‰¾åˆ°è®ºæ–‡: {total_found} ç¯‡")
        logger.info(f"  - æˆåŠŸå¤„ç†: {total_processed} ç¯‡")
        logger.info(f"  - ç›¸å…³æ€§è¿‡æ»¤: {total_filtered} ç¯‡")
        logger.info(f"  - å¤„ç†å¤±è´¥: {total_failed} ç¯‡")

        if filtered_papers:
            logger.info(f"ğŸ” è¢«è¿‡æ»¤çš„ä½ç›¸å…³æ€§è®ºæ–‡ (ç›¸å…³æ€§ < {self.min_relevance_score}):")
            for filtered in filtered_papers:
                logger.info(f"  - {filtered['paper_title']} (ID: {filtered['paper_id']}) - ç›¸å…³æ€§: {filtered['relevance_score']:.3f}")

        if failed_papers:
            logger.warning(f"âŒ å¤„ç†å¤±è´¥çš„è®ºæ–‡:")
            for failed in failed_papers:
                logger.warning(f"  - {failed['paper_title']} (ID: {failed['paper_id']}) - {failed['error_message']}")

        return processed_papers

    def _calculate_relevance_score(self, title: str, abstract: str) -> float:
        """
        è®¡ç®—è®ºæ–‡ä¸æœç´¢ä¸»é¢˜çš„ç›¸å…³æ€§åˆ†æ•°

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            abstract: è®ºæ–‡æ‘˜è¦

        Returns:
            ç›¸å…³æ€§åˆ†æ•° (0-1)
        """
        text = (title + " " + abstract).lower()
        score = 0.0

        # æ ¹æ®æœç´¢ä¸»é¢˜å®šä¹‰ä¸åŒçš„æƒé‡å…³é”®è¯
        if self.search_topic == 'VLM':
            # VLMé«˜æƒé‡å…³é”®è¯
            high_weight_keywords = ["vlm", "vision language model", "multimodal", "vision-language"]
            medium_weight_keywords = ["visual reasoning", "visual instruction", "image captioning", "visual grounding"]
            low_weight_keywords = ["cross-modal", "image-text", "visual understanding"]
        elif self.search_topic == 'VLA':
            # VLAé«˜æƒé‡å…³é”®è¯
            high_weight_keywords = ["vla", "vision language action", "embodied ai", "embodied agent"]
            medium_weight_keywords = ["robotic manipulation", "action planning", "visual navigation", "robot learning"]
            low_weight_keywords = ["policy learning", "motor control", "behavioral cloning"]
        else:  # BOTH
            # ç»¼åˆå…³é”®è¯
            high_weight_keywords = ["vlm", "vla", "vision language model", "vision language action", "embodied ai"]
            medium_weight_keywords = ["multimodal", "vision-language", "visual reasoning", "robotic manipulation"]
            low_weight_keywords = ["visual instruction", "image captioning", "action planning", "cross-modal"]

        # è®¡ç®—åˆ†æ•°
        for keyword in high_weight_keywords:
            if keyword in text:
                score += 0.3

        for keyword in medium_weight_keywords:
            if keyword in text:
                score += 0.2

        for keyword in low_weight_keywords:
            if keyword in text:
                score += 0.1

        return min(score, 1.0)

    def save_to_json(self, papers: List[Dict[str, Any]], filename: str = None) -> str:
        """
        ä¿å­˜è®ºæ–‡æ•°æ®åˆ°JSONæ–‡ä»¶

        Args:
            papers: å¤„ç†åçš„è®ºæ–‡æ•°æ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶åï¼Œé»˜è®¤ä½¿ç”¨æ—¥æœŸ

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            today = datetime.now().strftime("%Y-%m-%d")
            # topic_suffix = self.search_topic.lower()
            filename = f"arxiv_papers_{today}.json"

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "output"
        os.makedirs(output_dir, exist_ok=True)
        filepath = os.path.join(output_dir, filename)

        # æ„å»ºæœ€ç»ˆçš„JSONç»“æ„
        output_data = {
            "metadata": {
                "crawl_date": datetime.now().isoformat(),
                "total_papers": len(papers),
                "search_keywords": self.keywords,
                "search_topic": self.search_topic,
                "min_relevance_score": self.min_relevance_score,
                "data_source": "arxiv.org"
            },
            "papers": papers
        }

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            logger.info(f"æˆåŠŸä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ° {filepath}")
            return filepath

        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise

    def run_daily_crawl(self, days_back: int = 1) -> str:
        """
        æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡

        Args:
            days_back: çˆ¬å–è¿‡å»å‡ å¤©çš„è®ºæ–‡

        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥è®ºæ–‡çˆ¬å–ä»»åŠ¡...")

        try:
            # æœç´¢è®ºæ–‡
            papers = self.search_papers(days_back)

            if not papers:
                logger.warning("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
                return None

            # å¤„ç†è®ºæ–‡
            processed_papers = self.process_papers(papers)

            # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
            processed_papers.sort(key=lambda x: x['relevance_score'], reverse=True)

            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            output_file = self.save_to_json(processed_papers)

            logger.info(f"æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå…±å¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡")
            return output_file

        except Exception as e:
            logger.error(f"æ‰§è¡Œçˆ¬å–ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    kimi_api_key = os.getenv('KIMI_API_KEY')

    if not kimi_api_key:
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ KIMI_API_KEY")
        return

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ArxivPaperCrawler(kimi_api_key)

    try:
        # æ‰§è¡Œæ¯æ—¥çˆ¬å–
        output_file = crawler.run_daily_crawl(days_back=3)

        if output_file:
            print(f"âœ… çˆ¬å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")

    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()