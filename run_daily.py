#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥å®šæ—¶è¿è¡Œè„šæœ¬
å¯ä»¥é…åˆcronæˆ–Windowsä»»åŠ¡è®¡åˆ’ç¨‹åºä½¿ç”¨
"""

import sys
import os
from datetime import datetime
from arxiv_paper_crawler import ArxivPaperCrawler
from config import Config
from setup_logging import get_logger

def main():
    """ä¸»å‡½æ•°"""
    logger = get_logger('daily_crawl')

    logger.info("=" * 50)
    logger.info(f"å¼€å§‹æ¯æ—¥è®ºæ–‡çˆ¬å–ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 50)

    # æ£€æŸ¥APIå¯†é’¥
    if not Config.KIMI_API_KEY:
        logger.error("âŒ æœªè®¾ç½®KIMI_API_KEYç¯å¢ƒå˜é‡")
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export KIMI_API_KEY='your_api_key'")
        sys.exit(1)

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = ArxivPaperCrawler(
            kimi_api_key=Config.KIMI_API_KEY,
            kimi_base_url=Config.KIMI_BASE_URL
        )

        # æ‰§è¡Œçˆ¬å–
        output_file = crawler.run_daily_crawl(days_back=Config.DAYS_BACK)

        if output_file:
            logger.info(f"âœ… æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
            logger.info(f"ğŸ“„ ç»“æœæ–‡ä»¶: {output_file}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            import json
            with open(output_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                total_papers = data['metadata']['total_papers']
                logger.info(f"ğŸ“Š å…±å¤„ç† {total_papers} ç¯‡è®ºæ–‡")
        else:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")

    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

    logger.info("=" * 50)
    logger.info("æ¯æ—¥ä»»åŠ¡ç»“æŸ")
    logger.info("=" * 50)

if __name__ == "__main__":
    main()